{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabena98/ML_project/blob/main/ensembleNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hShIS2xb7mR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import math\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import LeakyReLU\n",
        "LeakyReLU = LeakyReLU(alpha=0.1)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNCwtprTsD-E"
      },
      "outputs": [],
      "source": [
        "# Make scorer accuracy\n",
        "score_acc = make_scorer(accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFDvByz1Y77L"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "\n",
        "\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Euclidean distance loss\n",
        "    https://en.wikipedia.org/wiki/Euclidean_distance\n",
        "    :param y_true: TensorFlow/Theano tensor\n",
        "    :param y_pred: TensorFlow/Theano tensor of the same shape as y_true\n",
        "    :return: float\n",
        "    \"\"\"\n",
        "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F03wLhwaw8v2"
      },
      "outputs": [],
      "source": [
        "#CUP\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "# load the dataset\n",
        "cup_train = pd.read_csv('/content/ML-CUP22-TR_noHeader.csv', names=[\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"y0\", \"y1\"]\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v31Milgw80y"
      },
      "outputs": [],
      "source": [
        "cup_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVOYG--Kw84Y"
      },
      "outputs": [],
      "source": [
        "cup_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3op3HgwU-tAF"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(cup_train.drop(columns=['y0','y1'], axis=0),\n",
        "                                                  cup_train[['y0', 'y1']],\n",
        "                                                  test_size=0.4, random_state=111)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gridsearch"
      ],
      "metadata": {
        "id": "cXvctMaIzStG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlZDghjgkgTI"
      },
      "outputs": [],
      "source": [
        "def find_layer_nodes_linear(n_layers, first_layer_nodes, last_layer_nodes):\n",
        "    layers = []\n",
        "    \n",
        "    nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "    nodes = first_layer_nodes\n",
        "    for i in range(1, n_layers+1):\n",
        "        layers.append(math.ceil(nodes))\n",
        "        nodes = nodes + nodes_increment\n",
        "    \n",
        "    return layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk9GIi3ngj26"
      },
      "outputs": [],
      "source": [
        "find_layer_nodes_linear(4,15,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHCs3EtDkgLc"
      },
      "outputs": [],
      "source": [
        "def createmodel(n_layers, first_layer_nodes, last_layer_nodes, learning_rate, momentum, activation_func, loss_func):\n",
        "    model = Sequential()\n",
        "    n_nodes = find_layer_nodes_linear(n_layers, first_layer_nodes, last_layer_nodes)\n",
        "    for i in range(1, n_layers):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=9, activation=activation_func))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=activation_func))\n",
        "            \n",
        "    #Finally, the output layer should have a single node in binary classification\n",
        "    model.add(Dense(2, activation=\"linear\"))\n",
        "    opt = tf.keras.optimizers.experimental.SGD(learning_rate, momentum=momentum)\n",
        "    model.compile(optimizer=opt, loss=loss_func, metrics = [\"mse\"]) #note: metrics could also be 'mse'\n",
        "    \n",
        "    return model\n",
        "\n",
        "##Wrap model into scikit-learn\n",
        "model =  KerasRegressor(build_fn=createmodel, verbose = False)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsGKmm5ekgGI"
      },
      "outputs": [],
      "source": [
        "activation_funcs = ['relu', 'elu'] \n",
        "loss_funcs = [euclidean_distance_loss]\n",
        "param_grid = dict(n_layers=[4,5], first_layer_nodes = [15, 25], last_layer_nodes = [2],  learning_rate = [0.01, 0.1], momentum = [0.01, 0.1],\n",
        "                  activation_func = activation_funcs, loss_func = loss_funcs, batch_size = [5,20], epochs = [250])\n",
        "grid = GridSearchCV(estimator = model, param_grid = param_grid, scoring=\"neg_mean_absolute_percentage_error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWU1U2fTkfxf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctJeIdbbszXg"
      },
      "outputs": [],
      "source": [
        "print(grid.best_score_)\n",
        "print(grid.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zczYpLoXszeo"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']].to_csv('grid_search_models_output.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY_fu22u7y5K"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(grid.cv_results_).columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "#cross validation\n",
        "\n",
        "fold_no = 1\n",
        "\n",
        "# Define per-fold score containers model 1\n",
        "mse_per_fold = []\n",
        "loss_per_fold = []\n",
        "loss_per_fold_train = []\n",
        "\n",
        "\n",
        "# Define per-fold score containers model 2\n",
        "mse_per_fold_2 = []\n",
        "loss_per_fold_2 = []\n",
        "loss_per_fold_train_2 = []\n",
        "\n",
        "# Define per-fold score containers ensemble \n",
        "loss_per_fold_ensemble_train = [] \n",
        "loss_per_fold_ensemble_val = []\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=5, shuffle=False)\n",
        "for train, test in kfold.split(X_train, y_train):\n",
        "  print(\"------------\")\n",
        "  print(\"MODEL scores\")\n",
        "  # define the keras model 1 for cup dataset\n",
        "  model_cup_1_cv = Sequential()\n",
        "  model_cup_1_cv.add(Dense(15, input_shape=(9,), activation='elu'))\n",
        "  model_cup_1_cv.add(Dense(11, activation='elu'))\n",
        "  model_cup_1_cv.add(Dense(7, activation='elu'))\n",
        "  model_cup_1_cv.add(Dense(3, activation='elu'))\n",
        "  model_cup_1_cv.add(Dense(2, activation='linear'))\n",
        "\n",
        "  # compile the keras model 1\n",
        "  opt = tf.keras.optimizers.experimental.SGD(0.01, momentum=0.1)\n",
        "  model_cup_1_cv.compile(optimizer=opt, loss=euclidean_distance_loss, metrics = [\"mse\"])\n",
        "\n",
        "  # fit the keras model 1 on the dataset\n",
        "  cv_1_cup = model_cup_1_cv.fit(X_train.iloc[train], y_train.iloc[train], validation_data=(X_train.iloc[test].values, y_train.iloc[test].values), epochs=250, batch_size=20, verbose=0)\n",
        "  # Generate generalization metrics\n",
        "  scores_1 = model_cup_1_cv.evaluate(X_train.iloc[test], y_train.iloc[test], verbose=0)\n",
        "  print(f'Model 1 - Score for fold {fold_no}: val {model_cup_1_cv.metrics_names[0]} of {round(scores_1[0],2)}; {model_cup_1_cv.metrics_names[1]} of {round(scores_1[1],1)}')\n",
        "  mse_per_fold.append(scores_1[1] * 100)\n",
        "  loss_per_fold.append(scores_1[0])\n",
        "  loss_per_fold_train.append(cv_1_cup.history['loss'][-1])\n",
        "\n",
        "\n",
        "  # define the keras model 2 for cup dataset\n",
        "  model_cup_2_cv = Sequential()\n",
        "  model_cup_2_cv.add(Dense(25, input_shape=(9,), activation='elu'))\n",
        "  model_cup_2_cv.add(Dense(18, activation='elu'))\n",
        "  model_cup_2_cv.add(Dense(10, activation='elu'))\n",
        "  model_cup_2_cv.add(Dense(2, activation='elu'))\n",
        "  model_cup_2_cv.add(Dense(2, activation='linear'))\n",
        "\n",
        "  # compile the keras model 1\n",
        "  opt = tf.keras.optimizers.experimental.SGD(0.01, momentum=0.01)\n",
        "  model_cup_2_cv.compile(optimizer=opt, loss=euclidean_distance_loss, metrics = [\"mse\"])\n",
        "\n",
        "  # fit the keras model 1 on the dataset\n",
        "  cv_2_cup = model_cup_2_cv.fit(X_train.iloc[train], y_train.iloc[train], validation_data=(X_train.iloc[test].values, y_train.iloc[test].values), epochs=250, batch_size=20, verbose=0)\n",
        "  # Generate generalization metrics\n",
        "  scores = model_cup_2_cv.evaluate(X_train.iloc[test], y_train.iloc[test], verbose=0)\n",
        "  print(f'Model 2 - Score for fold {fold_no}: val {model_cup_2_cv.metrics_names[0]} of {round(scores[0],2)}; {model_cup_2_cv.metrics_names[1]} of {round(scores[1],1)}')\n",
        "  mse_per_fold_2.append(scores[1] * 100)\n",
        "  loss_per_fold_2.append(scores[0])\n",
        "  loss_per_fold_train_2.append(cv_2_cup.history['loss'][-1])\n",
        "\n",
        "\n",
        "  print(\"-----Ensemble scores------\")\n",
        "\n",
        "  #averaging models making an ensemble\n",
        "  models=[model_cup_1_cv, model_cup_2_cv]\n",
        "  yhats = [model.predict(X_train.iloc[train], verbose=0) for model in models]\n",
        "  averaged_predictions = (yhats[1]+yhats[0])/2\n",
        "  euclidean_loss_ensemble = np.mean(euclidean_distance_loss(averaged_predictions, y_train.iloc[train].values))\n",
        "  print(f\"Fold {fold_no}: MEE of predictions on 4 folds--> {round(euclidean_loss_ensemble,2)}\")\n",
        "  loss_per_fold_ensemble_train.append(round(euclidean_loss_ensemble,2))\n",
        "\n",
        "  #averaging models making an ensemble\n",
        "  models=[model_cup_1_cv, model_cup_2_cv]\n",
        "  yhats = [model.predict(X_train.iloc[test], verbose=0) for model in models]\n",
        "  averaged_predictions = (yhats[1]+yhats[0])/2\n",
        "  euclidean_loss_ensemble = np.mean(euclidean_distance_loss(averaged_predictions, y_train.iloc[test].values))\n",
        "  print(f\"Fold {fold_no}: MEE of predictions on 1 fold--> {round(euclidean_loss_ensemble,2)}\")\n",
        "  loss_per_fold_ensemble_val.append(round(euclidean_loss_ensemble,2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1"
      ],
      "metadata": {
        "id": "dIDrIplyTArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MODEL 1 - CV\")\n",
        "avg_mee_cv_model_1 = round(sum(loss_per_fold) / len(loss_per_fold), 2)\n",
        "avg_mee_cv_model_1_train = round(sum(loss_per_fold_train) / len(loss_per_fold_train), 2)\n",
        "print(f\"The average euclidean error for the CV of model 1 is {avg_mee_cv_model_1} (for validation split)\")\n",
        "print(f\"The average euclidean error for the CV of model 1 is {avg_mee_cv_model_1_train} (for train split)\")\n",
        "\n",
        "print(\"MODEL 2 - CV\")\n",
        "avg_mee_cv_model_2 = round(sum(loss_per_fold_2) / len(loss_per_fold_2), 2)\n",
        "avg_mee_cv_model_2_train = round(sum(loss_per_fold_train_2) / len(loss_per_fold_train_2), 2)\n",
        "print(f\"The average euclidean error for the CV of model 2 is {avg_mee_cv_model_2} (for validation split)\")\n",
        "print(f\"The average euclidean error for the CV of model 2 is {avg_mee_cv_model_2_train} (for train split)\")\n",
        "\n",
        "print(\"Ensemble - CV\")\n",
        "avg_mee_cv_ensemble = round(sum(loss_per_fold_ensemble_val) / len(loss_per_fold_ensemble_val), 2)\n",
        "avg_mee_cv_ensemble_train = round(sum(loss_per_fold_ensemble_train) / len(loss_per_fold_ensemble_train), 2)\n",
        "print(f\"The average euclidean error for the CV of the ensemble is {avg_mee_cv_ensemble} (for validation split)\")\n",
        "print(f\"The average euclidean error for the CV of the ensemble is {avg_mee_cv_ensemble_train} (for train split)\")"
      ],
      "metadata": {
        "id": "KIOPgyFrouML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create ensemble with 2 different models with params from gridsearch"
      ],
      "metadata": {
        "id": "HwozW6cazBKI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAB8-a4LpsqM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "...\n",
        "# define the keras model 1 for cup dataset\n",
        "model_cup_1 = Sequential()\n",
        "model_cup_1.add(Dense(15, input_shape=(9,), activation='elu'))\n",
        "model_cup_1.add(Dense(11, activation='elu'))\n",
        "model_cup_1.add(Dense(7, activation='elu'))\n",
        "model_cup_1.add(Dense(3, activation='elu'))\n",
        "model_cup_1.add(Dense(2, activation='linear'))\n",
        "\n",
        "# compile the keras model 1\n",
        "opt = tf.keras.optimizers.experimental.SGD(0.01, momentum=0.1)\n",
        "model_cup_1.compile(optimizer=opt, loss=euclidean_distance_loss, metrics = [\"mse\"])\n",
        "\n",
        "# fit the keras model 1 on the dataset\n",
        "history_1_cup = model_cup_1.fit(X_train, y_train,validation_data=(X_val.values, y_val.values) , epochs=250, batch_size=20)\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history_1_cup.history['loss'])\n",
        "plt.plot(history_1_cup.history['val_loss'], linestyle=\"dashed\")\n",
        "plt.axis([0, 250, 1.3, 2.25])\n",
        "plt.title('model 1 loss - CUP')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('cup_member_1.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y5LEfW9gUqoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A-ZS6w06Ud2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "...\n",
        "# define the keras model 2 for cup dataset\n",
        "model_cup_2 = Sequential()\n",
        "model_cup_2.add(Dense(25, input_shape=(9,), activation='elu'))\n",
        "model_cup_2.add(Dense(18, activation='elu'))\n",
        "model_cup_2.add(Dense(10, activation='elu'))\n",
        "model_cup_2.add(Dense(2, activation='elu'))\n",
        "model_cup_2.add(Dense(2, activation='linear'))\n",
        "\n",
        "# compile the keras model 1\n",
        "opt = tf.keras.optimizers.experimental.SGD(0.01, momentum=0.01)\n",
        "model_cup_2.compile(optimizer=opt, loss=euclidean_distance_loss, metrics = [\"mse\"])\n",
        "\n",
        "# fit the keras model 1 on the dataset\n",
        "history_2_cup = model_cup_2.fit(X_train, y_train, validation_data=(X_val.values, y_val.values), epochs=250, batch_size=20)\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEB-eC0N-uhP"
      },
      "outputs": [],
      "source": [
        "plt.plot(history_1_cup.history['loss'])\n",
        "plt.plot(history_1_cup.history['val_loss'], linestyle=\"dashed\")\n",
        "plt.axis([0, 250, 1.3, 3.5])\n",
        "plt.title('model 2 loss - CUP')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.savefig('cup_member_2.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiJKfBSm74pH"
      },
      "outputs": [],
      "source": [
        "#averaging models making an ensemble\n",
        "models=[model_cup_1, model_cup_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2cOuVu58NC3"
      },
      "outputs": [],
      "source": [
        "yhats = [model.predict(X_val, verbose=0) for model in models]\n",
        "averaged_predictions = (yhats[1]+yhats[0])/2\n",
        "euclidean_loss_ensemble = np.mean(euclidean_distance_loss(averaged_predictions, y_val.values))\n",
        "euclidean_loss_model_1_test = np.mean(euclidean_distance_loss(yhats[0], y_val.values))\n",
        "euclidean_loss_model_2_test = np.mean(euclidean_distance_loss(yhats[1], y_val.values))\n",
        "print(f\"Euclidean loss ensemble {round(euclidean_loss_ensemble,2)}\")\n",
        "print(f\"Euclidean loss model 1 {round(euclidean_loss_model_1_test,2)}\")\n",
        "print(f\"Euclidean loss model 2 {round(euclidean_loss_model_2_test,2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CUP final prediction"
      ],
      "metadata": {
        "id": "8jzcQnY1zJvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxlenHdFOTH2"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "cup_train = pd.read_csv('/content/ML-CUP22-TS_noHeader.csv', names=[\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"y0\", \"y1\"]\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhats_cup_final = [model.predict(X_val) for model in models]\n",
        "averaged_predictions_cup_final = (yhats_cup_final[1]+yhats_cup_final[0])/2"
      ],
      "metadata": {
        "id": "5mLDCIPZ3-DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(averaged_predictions_cup_final, columns=[\"y0\", \"y1\"]).to_csv(\"averaged_predictions_cup_final_ensemble.csv\", index=False)"
      ],
      "metadata": {
        "id": "6tpvhQgQih_R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}